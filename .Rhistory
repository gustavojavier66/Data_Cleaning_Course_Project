install.packages("devtools")
library(devtools)
find rtools()
find_rtools()
data <- read.csv(hw1_data.csv)
data <- read.csv(C:\Users\cs0003000\Desktop\hw1_data.csv)
data <- read.csv("C:\Users\cs0003000\Desktop\hw1_data.csv")
data <- read.csv(file=C:\Users\cs0003000\Desktop\hw1_data.csv)
data <- read.csv(file="C:\Users\cs0003000\Desktop\hw1_data.csv")
read.csv()
file = C:\Users\cs0003000\Desktop\hw1_data.csv
file = "C:\Users\cs0003000\Desktop\hw1_data.csv""
data <- read.csv(hw1_data.csv)
data <- read.table(hw1_data.csv)
data <- read.csv(hw1_data.csv)
data <- read.csv("hw1_data.csv")
data
data[1:2,]
data[152:153,]
data[47,]
good <- complete.cases(data(Ozone))
data[ozone]
data[,2]
data[,4]
data.matrix()
data.matrix(data)
subset(DF, !is.na(Ozone))
subset(Data, !is.na(Ozone))
subset(data, !is.na(Ozone))
nrow(subset(data, !is.na(Ozone)))
nrow(data)
sum((subset(data, !is.na(Ozone)))$Ozone)/nrow(subset(data, !is.na(Ozone)))
sum((subset(data, !is.na(Ozone)))$Ozone)
subset(data,ozone>31)
subset(data,Ozone>31)
subset((subset(data,Ozone>31)),Temp >90)
nrows(subset((subset(data,Ozone>31)),Temp >90))
nrow(subset((subset(data,Ozone>31)),Temp >90))
sum((subset((subset((subset(data,Ozone>31)),Temp >90)))))/nrow(subset((subset(data,Ozone>31)),Temp >90))
sum((subset((subset((subset(data,Ozone>31)),Temp >90))))$Solar.R)/nrow(subset((subset(data,Ozone>31)),Temp >90))
subset(data, month=6)
sum(subset(data, month=6)$Temp)
sum(subset(data, month=6)$Temp)/nrow(subset(data, month=6))
sum(subset(data, Month=6)$Temp)
sum(subset(data, Month==6)$Temp)
subset(data, Month==6)$Temp
subset(data, Month==6)
sum(subset(data, Month==6)$Temp)
sum(subset(data, Month==6)$Temp)/nrow(subset(data, Month == 6))
subset(data, Month==5)
colmax(subset(data, Month==5))
colMax(subset(data, Month==5))
sort(data$Ozone)
sort(subset(data, Month==5)$Ozone)
..
savehistory("~/rstudio 1.Rhistory")
savehistory("~/rhistory 1.txt")
cube <- function(x, n) {
x^3
}
cube <- function(x, n) {
x^3
}
cube(3)
x <- 1:10
if(x > 5) {
x <- 0
}
f <- function(x) {
g <- function(y) {
y + z
}
z <- 4
x + g(x)
}
f <- function(x) {
g <- function(y) {
y + z
}
z <- 4
x + g(x)
}
z<-10
f(3)
x <- 5
y <- if(x < 3) {
NA
} else {
10
}
x <- 5
y <- if(x < 3) {
NA
} else {
10
}
x <- 5
y <- if(x < 3) {
NA
} else {
10
}
y
sprint("%02d",7)
sprintf("%02d",7)
sprintf("%020d",7)
sprintf("%020d",227)
## Created a Class makeCacheMatrix which takes a matrix and allows the user to
## create an Object that will store it's inverse
## makeCacheMAtrix class contains a matrix and also allows for storage of the
## inverse of the matrix
makeCacheMatrix <- function(x = matrix()) {
inv <- NULL
set <- function(y) {
x <<- y
inv <<- NULL
}
get <- function() x
setinverse <- function(inverse) inv <<- inverse
getinverse <- function() inv
list(
set=set,
get=get,
setinverse=setinverse,
getinverse=getinverse
)
}
## cacheSolve determines if the makeCacheMatrix object received has a inverse
## matrix stored.  If it doesn't have it stored it will compute the inverse
## and return it.  If it does have it stored it will return inverse as found in the
## inv field.
cacheSolve <- function(x, ...) {
inv <- x$getinverse()
if(!is.null(inv)) {
message("getting cached data.")
return(inv)
}
data <- x$get()
inv <- solve(data)
x$setinverse(inv)
inv
}
invmat <- matrix(cbind(1,1), cbind(1,1))
invmat
invmat = rbind(c(1, -1/4), c(1/4,1))
invmat
makeCacheMatrix(invmat)
m = makeCacheMatrix(invmat)
m
m$get()
cacheSolve(m)
cacheSolve(m)
setwd(./r programming)
setwd("./r programming/class 3 Data cleaning")
setwd("./Quiz 4")
setwd("./Quiz 4")
library(dplyr)
install.packages(dplyr)
install.packages("dplyr")
install.packages(dplyr)
library(dplyr)
if(!file.exists("./data")){dir.create("./data")}
fileUrl1 <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
fileUrl2 <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FPUMSDataDict06.pdf"
if(!file.exists("./data/housing_idaho.csv")){download.file(fileUrl1,destfile="./data/housing_idaho.csv",method="curl")}
if(!file.exists("./data/cookbook.pdf")){download.file(fileUrl2,destfile="./data/cookbook.pdf",method="curl")}
housingdata <- read.csv("./data/housing_idaho.csv")
list <-strsplit(names(housingdata),split="wgtp")
list[[123]]
if(!file.exists("./data")){dir.create("./data")}
fileUrl1 <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
fileUrl2 <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FPUMSDataDict06.pdf"
if(!file.exists("./data/housing_idaho.csv")){download.file(fileUrl1,destfile="./data/housing_idaho.csv")}
if(!file.exists("./data/cookbook.pdf")){download.file(fileUrl2,destfile="./data/cookbook.pdf",method="curl")}
if(!file.exists("./data/cookbook.pdf")){download.file(fileUrl2,destfile="./data/cookbook.pdf")}
housingdata <- read.csv("./data/housing_idaho.csv")
list <-strsplit(names(housingdata),split="wgtp")
list[[123]]
fileUrl1 <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv"
if(!file.exists("./data/gdp.csv")){download.file(fileUrl1,destfile="./data/gdp.csv")}
data <- read.csv("./data/gdp.csv",skip=4, nrows=190)
cleanedData <- gsub(",","",data$X.4)
cleanedData <- (as.numeric(cleanedData))
mean(cleanedData,na.rm = TRUE)
grep("^United",countryNames), 4
countryNames <- data$X.3
grep("^United",countryNames), 4
grep("^United",countryNames), 3
grep("United$",countryNames), 3
grep("*United",countryNames), 2
countryNames
grep("^United",countryNames), 4
grep("^United", countryNames), 4
grep("^United", countryNames)
fileUrl1 <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv"
if(!file.exists("./data/gdpdata.csv")){download.file(fileUrl1,destfile="./data/gdpdata.csv"")}
fileUrl1 <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv"
if(!file.exists("./data/gdpdata.csv")){download.file(fileUrl1,destfile="./data/gdpdata.csv"")}
kj
oijoj
''
fileUrl1 <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv"
i
fileUrl1 <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv"
if(!file.exists("./data/gdpdata.csv")){download.file(fileUrl1,destfile="./data/gdpdata.csv")}
fileUrl1 <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FEDSTATS_Country.csv"
if(!file.exists("./data/educationaldata.csv")){download.file(fileUrl1,destfile="./data/educationaldata.csv")}
gdpdata <- read.csv("./data/gdpdata.csv",skip=4,nrows=190)
educationaldata <- read.csv("./data/educationaldata.csv")
mergedData <- merge(gdpdata,educationaldata,by.x="X", by.y="CountryCode")
fy.june <- grep('Fiscal year end: June', mergedData$Special.Notes)
length(fy.june)
fy.june
mergedData
head(mergedData)
head(mergedData$Special.Notes)
library(quantmod)
amzn = getSymbols("AMZN",auto.assign=FALSE)
sampleTimes = index(amzn)
install.packages(quantmod)
install.packages("quantmod")
library(quantmod)
amzn = getSymbols("AMZN",auto.assign=FALSE)
sampleTimes = index(amzn)
library(quantmod)
amzn = getSymbols("AMZN",auto.assign=FALSE)
sampleTimes = index(amzn)
head(sampleTimes)
grep('^2012', sampleTimes)
length(grep('^2012', sampleTimes))
sampleTimes
sampleTimes2012 <- subset(sampleTimes, year2012)
day <- format(sampleTimes2012, '%A')
table(day)
day <- format(sampleTimes2012, '%A')
table(day)
fromat((grep('^2012', sampleTimes)), '%A')
format((grep('^2012', sampleTimes)), '%A')
grep('^2012', sampleTimes)
samples <- subset(sampleTimes, grepl('2012-*', sampleTimes))
samples
day <- format(samples, '@A')
table(day)
day <- format(samples, '$A')
table(day)
sampleTimes = index(amzn)
year2012 <- grepl('2012-*', sampleTimes)
sampleTimes2012 <- subset(sampleTimes, year2012)
day <- format(sampleTimes2012, '%A')
table(day)
getwd()
setwd("..")
getwd()
setwd("./course project")
library(plyr)
library(data.table)
install.packages("plyr","data.table")
install.packages("plyr", "data.table")
install.packages("plyr")
library(plyr)
library(data.table)
subjectTrain = read.table('./train/subject_train.txt',header=FALSE)
xTrain = read.table('./train/x_train.txt',header=FALSE)
yTrain = read.table('./train/y_train.txt',header=FALSE)
subjectTest = read.table('./test/subject_test.txt',header=FALSE)
xTest = read.table('./test/x_test.txt',header=FALSE)
yTest = read.table('./test/y_test.txt',header=FALSE)
subjectTrain = read.table('subject_train.txt',header=FALSE)
library(data.table)
# 0. load test and training sets and the activities
# Use the course CDN instead of the original UCI zip file.
#fileUrl <- "http://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip"
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip"
download.file(fileUrl, destfile = "Dataset.zip", method = "curl")
unzip("Dataset.zip")
testData <- read.table("./UCI HAR Dataset/test/X_test.txt",header=FALSE)
testData_act <- read.table("./UCI HAR Dataset/test/y_test.txt",header=FALSE)
testData_sub <- read.table("./UCI HAR Dataset/test/subject_test.txt",header=FALSE)
trainData <- read.table("./UCI HAR Dataset/train/X_train.txt",header=FALSE)
trainData_act <- read.table("./UCI HAR Dataset/train/y_train.txt",header=FALSE)
trainData_sub <- read.table("./UCI HAR Dataset/train/subject_train.txt",header=FALSE)
# 3. Uses descriptive activity names to name the activities in the data set
activities <- read.table("./UCI HAR Dataset/activity_labels.txt",header=FALSE,colClasses="character")
testData_act$V1 <- factor(testData_act$V1,levels=activities$V1,labels=activities$V2)
trainData_act$V1 <- factor(trainData_act$V1,levels=activities$V1,labels=activities$V2)
# 4. Appropriately labels the data set with descriptive activity names
features <- read.table("./UCI HAR Dataset/features.txt",header=FALSE,colClasses="character")
colnames(testData)<-features$V2
colnames(trainData)<-features$V2
colnames(testData_act)<-c("Activity")
colnames(trainData_act)<-c("Activity")
colnames(testData_sub)<-c("Subject")
colnames(trainData_sub)<-c("Subject")
# 1. merge test and training sets into one data set, including the activities
testData<-cbind(testData,testData_act)
testData<-cbind(testData,testData_sub)
trainData<-cbind(trainData,trainData_act)
trainData<-cbind(trainData,trainData_sub)
bigData<-rbind(testData,trainData)
# 2. extract only the measurements on the mean and standard deviation for each measurement
bigData_mean<-sapply(bigData,mean,na.rm=TRUE)
bigData_sd<-sapply(bigData,sd,na.rm=TRUE)
# 5. Creates a second, independent tidy data set with the average of each variable for each activity and each subject.
DT <- data.table(bigData)
tidy<-DT[,lapply(.SD,mean),by="Activity,Subject"]
write.table(tidy,file="tidy.csv",sep=",",row.names = FALSE)
source("run_analysis")
source("run_analysis.r")
library(data.table)
# 0. load test and training sets and the activities
# Use the course CDN instead of the original UCI zip file.
#fileUrl <- "http://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip"
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip"
download.file(fileUrl, destfile = "Dataset.zip")
unzip("Dataset.zip")
testData <- read.table("./UCI HAR Dataset/test/X_test.txt",header=FALSE)
testData_act <- read.table("./UCI HAR Dataset/test/y_test.txt",header=FALSE)
testData_sub <- read.table("./UCI HAR Dataset/test/subject_test.txt",header=FALSE)
trainData <- read.table("./UCI HAR Dataset/train/X_train.txt",header=FALSE)
trainData_act <- read.table("./UCI HAR Dataset/train/y_train.txt",header=FALSE)
trainData_sub <- read.table("./UCI HAR Dataset/train/subject_train.txt",header=FALSE)
# 3. Uses descriptive activity names to name the activities in the data set
activities <- read.table("./UCI HAR Dataset/activity_labels.txt",header=FALSE,colClasses="character")
testData_act$V1 <- factor(testData_act$V1,levels=activities$V1,labels=activities$V2)
trainData_act$V1 <- factor(trainData_act$V1,levels=activities$V1,labels=activities$V2)
# 4. Appropriately labels the data set with descriptive activity names
features <- read.table("./UCI HAR Dataset/features.txt",header=FALSE,colClasses="character")
colnames(testData)<-features$V2
colnames(trainData)<-features$V2
colnames(testData_act)<-c("Activity")
colnames(trainData_act)<-c("Activity")
colnames(testData_sub)<-c("Subject")
colnames(trainData_sub)<-c("Subject")
# 1. merge test and training sets into one data set, including the activities
testData<-cbind(testData,testData_act)
testData<-cbind(testData,testData_sub)
trainData<-cbind(trainData,trainData_act)
trainData<-cbind(trainData,trainData_sub)
bigData<-rbind(testData,trainData)
# 2. extract only the measurements on the mean and standard deviation for each measurement
bigData_mean<-sapply(bigData,mean,na.rm=TRUE)
bigData_sd<-sapply(bigData,sd,na.rm=TRUE)
# 5. Creates a second, independent tidy data set with the average of each variable for each activity and each subject.
DT <- data.table(bigData)
tidy<-DT[,lapply(.SD,mean),by="Activity,Subject"]
write.table(tidy,file="tidy.csv",sep=",",row.names = FALSE)
